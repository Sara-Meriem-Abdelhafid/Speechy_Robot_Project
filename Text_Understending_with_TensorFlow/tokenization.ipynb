{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Love my dog\n",
      "انا احب كلبي\n",
      "انا احب قطتي\n",
      "I Love my cat\n",
      "You love my dog!\n",
      "Do you think my dog is amazing?\n",
      "hello Speechy\n",
      "hello how are you\n",
      "if you had a little again has better\n",
      "كيف اقول بانني احبك\n",
      "هل يمكنني ان اسالك سؤالا\n",
      "مرحبا بك\n",
      "مرحبا كيف حالك\n",
      "هل انت بخير\n",
      "اخصها من هنا جوجل نخدم بالمكتبه تاع جوجل هذه\n",
      "ما هي\n",
      "بغيتي تجيبي حاجه\n",
      "اول مره اول مره\n",
      "اول\n",
      "كوبريت بالليل باللي يفهم باللي يفهم لهجات\n",
      "عيد الكافيه الحلوه\n",
      "اخصها من هنا جوجل نخدم بالمكتبه تاع جوجل هذه اللي كاتهضر فيها عيد\n",
      "علاه راكي تسقسي في\n",
      "شو بغيت نفهم\n",
      "بياع فرنسا\n",
      "لدرجه انه بدا ينزف فجن خيط بيضات وكشف عليها وعرف انه ما عندوش قضايا خالص فبيطلب\n",
      "كيف حالك هل انت بخير\n",
      "مرحبا\n",
      "كيف يبدو\n",
      "هل يمكنني ان اسالك\n",
      "هل انت بخير\n",
      "هل يمكنني ان\n",
      "هل يمكنني ان\n",
      "هل يمكنني\n",
      "ولكن لماذا\n",
      "انت\n",
      "كيف تسيل ام\n",
      "كيف يمكنني ان اسالك\n",
      "هل لي بسؤال معين اسالك اياه\n",
      "مرحبا بك\n",
      "مرحبا بكسبيشي\n",
      "مرحبا بك\n",
      "مرحبا سبيتش\n",
      "مرحبا\n",
      "كيف حال اكس بيتشي\n",
      "السلام\n"
     ]
    }
   ],
   "source": [
    "# Existing list of sentences\n",
    "sentences = [\n",
    "    'I Love my dog',\n",
    "    'انا احب كلبي',\n",
    "    'انا احب قطتي',\n",
    "    'I Love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?']\n",
    "\n",
    "# Read sentences from a text file and append to the list\n",
    "file_path = \"/Users/saraabdelhafid/Desktop/My Kids Robot/Source Code/Speechy_Robot/IA/text_to_speech/audio_to_text_output.txt\" # path to the speech_to_text file captured by the robot \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    new_sentences = [line.strip() for line in file.readlines() if line.strip()]  # Add Sentences to the sentences list without adding the empty lines\n",
    "    sentences.extend(new_sentences)\n",
    "\n",
    "# Print the updated list of sentences\n",
    "for sentence in sentences:\n",
    "    print(sentence) # Strip any leading or trailing whitespaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'هل': 2, 'مرحبا': 3, 'كيف': 4, 'يمكنني': 5, 'ان': 6, 'my': 7, 'you': 8, 'اسالك': 9, 'انت': 10, 'جوجل': 11, 'love': 12, 'dog': 13, 'بك': 14, 'بخير': 15, 'اول': 16, 'i': 17, 'انا': 18, 'احب': 19, 'hello': 20, 'حالك': 21, 'اخصها': 22, 'من': 23, 'هنا': 24, 'نخدم': 25, 'بالمكتبه': 26, 'تاع': 27, 'هذه': 28, 'ما': 29, 'مره': 30, 'باللي': 31, 'يفهم': 32, 'عيد': 33, 'انه': 34, 'كلبي': 35, 'قطتي': 36, 'cat': 37, 'do': 38, 'think': 39, 'is': 40, 'amazing': 41, 'speechy': 42, 'how': 43, 'are': 44, 'if': 45, 'had': 46, 'a': 47, 'little': 48, 'again': 49, 'has': 50, 'better': 51, 'اقول': 52, 'بانني': 53, 'احبك': 54, 'سؤالا': 55, 'هي': 56, 'بغيتي': 57, 'تجيبي': 58, 'حاجه': 59, 'كوبريت': 60, 'بالليل': 61, 'لهجات': 62, 'الكافيه': 63, 'الحلوه': 64, 'اللي': 65, 'كاتهضر': 66, 'فيها': 67, 'علاه': 68, 'راكي': 69, 'تسقسي': 70, 'في': 71, 'شو': 72, 'بغيت': 73, 'نفهم': 74, 'بياع': 75, 'فرنسا': 76, 'لدرجه': 77, 'بدا': 78, 'ينزف': 79, 'فجن': 80, 'خيط': 81, 'بيضات': 82, 'وكشف': 83, 'عليها': 84, 'وعرف': 85, 'عندوش': 86, 'قضايا': 87, 'خالص': 88, 'فبيطلب': 89, 'يبدو': 90, 'ولكن': 91, 'لماذا': 92, 'تسيل': 93, 'ام': 94, 'لي': 95, 'بسؤال': 96, 'معين': 97, 'اياه': 98, 'بكسبيشي': 99, 'سبيتش': 100, 'حال': 101, 'اكس': 102, 'بيتشي': 103, 'السلام': 104}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")#out of vocabilary\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'هل': 2, 'مرحبا': 3, 'كيف': 4, 'يمكنني': 5, 'ان': 6, 'my': 7, 'you': 8, 'اسالك': 9, 'انت': 10, 'جوجل': 11, 'love': 12, 'dog': 13, 'بك': 14, 'بخير': 15, 'اول': 16, 'i': 17, 'انا': 18, 'احب': 19, 'hello': 20, 'حالك': 21, 'اخصها': 22, 'من': 23, 'هنا': 24, 'نخدم': 25, 'بالمكتبه': 26, 'تاع': 27, 'هذه': 28, 'ما': 29, 'مره': 30, 'باللي': 31, 'يفهم': 32, 'عيد': 33, 'انه': 34, 'كلبي': 35, 'قطتي': 36, 'cat': 37, 'do': 38, 'think': 39, 'is': 40, 'amazing': 41, 'speechy': 42, 'how': 43, 'are': 44, 'if': 45, 'had': 46, 'a': 47, 'little': 48, 'again': 49, 'has': 50, 'better': 51, 'اقول': 52, 'بانني': 53, 'احبك': 54, 'سؤالا': 55, 'هي': 56, 'بغيتي': 57, 'تجيبي': 58, 'حاجه': 59, 'كوبريت': 60, 'بالليل': 61, 'لهجات': 62, 'الكافيه': 63, 'الحلوه': 64, 'اللي': 65, 'كاتهضر': 66, 'فيها': 67, 'علاه': 68, 'راكي': 69, 'تسقسي': 70, 'في': 71, 'شو': 72, 'بغيت': 73, 'نفهم': 74, 'بياع': 75, 'فرنسا': 76, 'لدرجه': 77, 'بدا': 78, 'ينزف': 79, 'فجن': 80, 'خيط': 81, 'بيضات': 82, 'وكشف': 83, 'عليها': 84, 'وعرف': 85, 'عندوش': 86, 'قضايا': 87, 'خالص': 88, 'فبيطلب': 89, 'يبدو': 90, 'ولكن': 91, 'لماذا': 92, 'تسيل': 93, 'ام': 94, 'لي': 95, 'بسؤال': 96, 'معين': 97, 'اياه': 98, 'بكسبيشي': 99, 'سبيتش': 100, 'حال': 101, 'اكس': 102, 'بيتشي': 103, 'السلام': 104}\n",
      "\n",
      "Sequences =  [[17, 12, 7, 13], [18, 19, 35], [18, 19, 36], [17, 12, 7, 37], [8, 12, 7, 13], [38, 8, 39, 7, 13, 40, 41], [20, 42], [20, 43, 44, 8], [45, 8, 46, 47, 48, 49, 50, 51], [4, 52, 53, 54], [2, 5, 6, 9, 55], [3, 14], [3, 4, 21], [2, 10, 15], [22, 23, 24, 11, 25, 26, 27, 11, 28], [29, 56], [57, 58, 59], [16, 30, 16, 30], [16], [60, 61, 31, 32, 31, 32, 62], [33, 63, 64], [22, 23, 24, 11, 25, 26, 27, 11, 28, 65, 66, 67, 33], [68, 69, 70, 71], [72, 73, 74], [75, 76], [77, 34, 78, 79, 80, 81, 82, 83, 84, 85, 34, 29, 86, 87, 88, 89], [4, 21, 2, 10, 15], [3], [4, 90], [2, 5, 6, 9], [2, 10, 15], [2, 5, 6], [2, 5, 6], [2, 5], [91, 92], [10], [4, 93, 94], [4, 5, 6, 9], [2, 95, 96, 97, 9, 98], [3, 14], [3, 99], [3, 14], [3, 1], [3], [4, 1, 1, 1], [1]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0 17 12  7 13]\n",
      " [ 0  0 18 19 35]\n",
      " [ 0  0 18 19 36]\n",
      " [ 0 17 12  7 37]\n",
      " [ 0  8 12  7 13]\n",
      " [39  7 13 40 41]\n",
      " [ 0  0  0 20 42]\n",
      " [ 0 20 43 44  8]\n",
      " [47 48 49 50 51]\n",
      " [ 0  4 52 53 54]\n",
      " [ 2  5  6  9 55]\n",
      " [ 0  0  0  3 14]\n",
      " [ 0  0  3  4 21]\n",
      " [ 0  0  2 10 15]\n",
      " [25 26 27 11 28]\n",
      " [ 0  0  0 29 56]\n",
      " [ 0  0 57 58 59]\n",
      " [ 0 16 30 16 30]\n",
      " [ 0  0  0  0 16]\n",
      " [31 32 31 32 62]\n",
      " [ 0  0 33 63 64]\n",
      " [28 65 66 67 33]\n",
      " [ 0 68 69 70 71]\n",
      " [ 0  0 72 73 74]\n",
      " [ 0  0  0 75 76]\n",
      " [29 86 87 88 89]\n",
      " [ 4 21  2 10 15]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  0  0  4 90]\n",
      " [ 0  2  5  6  9]\n",
      " [ 0  0  2 10 15]\n",
      " [ 0  0  2  5  6]\n",
      " [ 0  0  2  5  6]\n",
      " [ 0  0  0  2  5]\n",
      " [ 0  0  0 91 92]\n",
      " [ 0  0  0  0 10]\n",
      " [ 0  0  4 93 94]\n",
      " [ 0  4  5  6  9]\n",
      " [95 96 97  9 98]\n",
      " [ 0  0  0  3 14]\n",
      " [ 0  0  0  3 99]\n",
      " [ 0  0  0  3 14]\n",
      " [ 0  0  0  3  1]\n",
      " [ 0  0  0  0  3]\n",
      " [ 0  4  1  1  1]\n",
      " [ 0  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)# matrix of same size\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sequences to the same Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Sequence =  [[17, 12, 7, 13], [18, 19, 35], [18, 19, 36], [17, 12, 7, 37], [8, 12, 7, 13], [38, 8, 39, 7, 13, 40, 41], [20, 42], [20, 43, 44, 8], [45, 8, 46, 47, 48, 49, 50, 51], [4, 52, 53, 54], [2, 5, 6, 9, 55], [3, 14], [3, 4, 21], [2, 10, 15], [22, 23, 24, 11, 25, 26, 27, 11, 28], [29, 56], [57, 58, 59], [16, 30, 16, 30], [16], [60, 61, 31, 32, 31, 32, 62], [33, 63, 64], [22, 23, 24, 11, 25, 26, 27, 11, 28, 65, 66, 67, 33], [68, 69, 70, 71], [72, 73, 74], [75, 76], [77, 34, 78, 79, 80, 81, 82, 83, 84, 85, 34, 29, 86, 87, 88, 89], [4, 21, 2, 10, 15], [3], [4, 90], [2, 5, 6, 9], [2, 10, 15], [2, 5, 6], [2, 5, 6], [2, 5], [91, 92], [10], [4, 93, 94], [4, 5, 6, 9], [2, 95, 96, 97, 9, 98], [3, 14], [3, 99], [3, 14], [3, 1], [3], [4, 1, 1, 1], [1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[ 0  0  0  0  0  0 17 12  7 13]\n",
      " [ 0  0  0  0  0  0  0 18 19 35]\n",
      " [ 0  0  0  0  0  0  0 18 19 36]\n",
      " [ 0  0  0  0  0  0 17 12  7 37]\n",
      " [ 0  0  0  0  0  0  8 12  7 13]\n",
      " [ 0  0  0 38  8 39  7 13 40 41]\n",
      " [ 0  0  0  0  0  0  0  0 20 42]\n",
      " [ 0  0  0  0  0  0 20 43 44  8]\n",
      " [ 0  0 45  8 46 47 48 49 50 51]\n",
      " [ 0  0  0  0  0  0  4 52 53 54]\n",
      " [ 0  0  0  0  0  2  5  6  9 55]\n",
      " [ 0  0  0  0  0  0  0  0  3 14]\n",
      " [ 0  0  0  0  0  0  0  3  4 21]\n",
      " [ 0  0  0  0  0  0  0  2 10 15]\n",
      " [ 0 22 23 24 11 25 26 27 11 28]\n",
      " [ 0  0  0  0  0  0  0  0 29 56]\n",
      " [ 0  0  0  0  0  0  0 57 58 59]\n",
      " [ 0  0  0  0  0  0 16 30 16 30]\n",
      " [ 0  0  0  0  0  0  0  0  0 16]\n",
      " [ 0  0  0 60 61 31 32 31 32 62]\n",
      " [ 0  0  0  0  0  0  0 33 63 64]\n",
      " [11 25 26 27 11 28 65 66 67 33]\n",
      " [ 0  0  0  0  0  0 68 69 70 71]\n",
      " [ 0  0  0  0  0  0  0 72 73 74]\n",
      " [ 0  0  0  0  0  0  0  0 75 76]\n",
      " [82 83 84 85 34 29 86 87 88 89]\n",
      " [ 0  0  0  0  0  4 21  2 10 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  4 90]\n",
      " [ 0  0  0  0  0  0  2  5  6  9]\n",
      " [ 0  0  0  0  0  0  0  2 10 15]\n",
      " [ 0  0  0  0  0  0  0  2  5  6]\n",
      " [ 0  0  0  0  0  0  0  2  5  6]\n",
      " [ 0  0  0  0  0  0  0  0  2  5]\n",
      " [ 0  0  0  0  0  0  0  0 91 92]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]\n",
      " [ 0  0  0  0  0  0  0  4 93 94]\n",
      " [ 0  0  0  0  0  0  4  5  6  9]\n",
      " [ 0  0  0  0  2 95 96 97  9 98]\n",
      " [ 0  0  0  0  0  0  0  0  3 14]\n",
      " [ 0  0  0  0  0  0  0  0  3 99]\n",
      " [ 0  0  0  0  0  0  0  0  3 14]\n",
      " [ 0  0  0  0  0  0  0  0  3  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  4  1  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "sentences\n",
    "\n",
    "\n",
    "#test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq = tokenizer.texts_to_sequences(sentences)#test_data\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "padded = pad_sequences(test_seq, maxlen=10)# matrix of same size\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
